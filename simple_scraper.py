#!/usr/bin/env python3
"""
Simplified Chiikawa scraper based on working debug approach
"""

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
from datetime import datetime
import argparse
import logging
from utils import clean_text, parse_price

def setup_logging(verbose=False):
    """ãƒ­ã‚®ãƒ³ã‚°è¨­å®šã‚’åˆæœŸåŒ–"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(levelname)s - %(message)s',
    )

def scrape_collection(collection_name, max_products=None):
    """æŒ‡å®šã•ã‚ŒãŸã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å•†å“ã‚’å–å¾—"""
    base_url = "https://chiikawamarket.jp"
    url = f"{base_url}/collections/{collection_name}?page=1"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ˜ç¤ºçš„ã«è¨­å®š
        response.encoding = response.apparent_encoding or 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å•†å“ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        product_links = soup.find_all('a', href=re.compile(rf'/collections/{collection_name}/products/'))
        
        logging.info(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã§ {len(product_links)} å€‹ã®å•†å“ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹")
        
        products = []
        
        for i, link in enumerate(product_links):
            if max_products and i >= max_products:
                break
                
            try:
                href = link.get('href', '')
                product_url = f"{base_url}{href}" if href.startswith('/') else href
                
                # å•†å“åã‚’å–å¾—
                title = clean_text(link.get_text()) if link.get_text().strip() else "å•†å“åä¸æ˜"
                
                # è¦ªè¦ç´ ã‹ã‚‰ä¾¡æ ¼æƒ…å ±ã‚’æ¢ã™
                parent = link.parent
                price = None
                
                # ä¾¡æ ¼ã‚’æ¢ã™ãŸã‚ã€è¦ªè¦ç´ ã‚’é¡ã‚‹
                current = parent
                for _ in range(5):  # æœ€å¤§5éšå±¤ã¾ã§é¡ã‚‹
                    if current:
                        price_text = current.get_text()
                        if 'Â¥' in price_text:
                            price = parse_price(price_text)
                            if price:
                                break
                        current = current.parent
                    else:
                        break
                
                # åœ¨åº«çŠ¶æ³ã‚’åˆ¤å®š
                stock_status = 'åœ¨åº«ã‚ã‚Š'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                parent_text = parent.get_text().lower() if parent else ''
                
                if any(pattern in parent_text for pattern in ['å£²ã‚Šåˆ‡ã‚Œ', 'sold out', 'å®Œå£²']):
                    stock_status = 'å£²ã‚Šåˆ‡ã‚Œ'
                elif 'äºˆç´„' in parent_text:
                    stock_status = 'äºˆç´„å•†å“'
                elif any(pattern in parent_text for pattern in ['new', 'æ–°ç€']):
                    stock_status = 'æ–°ç€å•†å“'
                
                # å•†å“IDã‚’æŠ½å‡º
                product_id = href.split('/')[-1] if href else f"product_{i+1}"
                
                product_data = {
                    'å•†å“ID': product_id,
                    'å•†å“å': title,
                    'å•†å“URL': product_url,
                    'ä¾¡æ ¼': price,
                    'ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³': collection_name,
                    'åœ¨åº«çŠ¶æ³': stock_status,
                    'å–å¾—æ—¥æ™‚': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                products.append(product_data)
                logging.debug(f"å•†å“ {i+1}: {title} - Â¥{price} ({stock_status})")
                
            except Exception as e:
                logging.warning(f"å•†å“ {i+1} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        return products
        
    except Exception as e:
        logging.error(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection_name}' ã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def save_to_csv(products, filename):
    """å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    if not products:
        logging.warning("ä¿å­˜ã™ã‚‹å•†å“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    df = pd.DataFrame(products)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    logging.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")

def save_to_excel(products, filename):
    """å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    if not products:
        logging.warning("ä¿å­˜ã™ã‚‹å•†å“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    df = pd.DataFrame(products)
    
    with pd.ExcelWriter(filename, engine='openpyxl') as writer:
        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ
        df.to_excel(writer, sheet_name='å•†å“ãƒ‡ãƒ¼ã‚¿', index=False)
        
        # çµ±è¨ˆã‚·ãƒ¼ãƒˆ
        stats_data = []
        stats_data.append(['é …ç›®', 'å€¤'])
        stats_data.append(['ç·å•†å“æ•°', len(df)])
        stats_data.append(['ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æ•°', df['ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³'].nunique()])
        
        # åœ¨åº«çŠ¶æ³åˆ†å¸ƒ
        stock_dist = df['åœ¨åº«çŠ¶æ³'].value_counts()
        stats_data.append(['', ''])
        stats_data.append(['åœ¨åº«çŠ¶æ³', 'å•†å“æ•°'])
        for status, count in stock_dist.items():
            stats_data.append([status, count])
        
        # ä¾¡æ ¼çµ±è¨ˆ
        if 'ä¾¡æ ¼' in df.columns and not df['ä¾¡æ ¼'].isna().all():
            stats_data.append(['', ''])
            stats_data.append(['ä¾¡æ ¼çµ±è¨ˆ', ''])
            stats_data.append(['å¹³å‡ä¾¡æ ¼', f"Â¥{df['ä¾¡æ ¼'].mean():.0f}"])
            stats_data.append(['æœ€é«˜ä¾¡æ ¼', f"Â¥{df['ä¾¡æ ¼'].max():.0f}"])
            stats_data.append(['æœ€ä½ä¾¡æ ¼', f"Â¥{df['ä¾¡æ ¼'].min():.0f}"])
        
        stats_df = pd.DataFrame(stats_data)
        stats_df.to_excel(writer, sheet_name='çµ±è¨ˆæƒ…å ±', index=False, header=False)
    
    logging.info(f"Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {filename}")

def main():
    parser = argparse.ArgumentParser(description='ã¡ã„ã‹ã‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒ¼ã‚±ãƒƒãƒˆ ç°¡æ˜“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«')
    parser.add_argument('--collections', '-c', default='newitems', help='å–å¾—ã™ã‚‹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)')
    parser.add_argument('--max-products', '-m', type=int, default=50, help='æœ€å¤§å–å¾—å•†å“æ•°')
    parser.add_argument('--format', '-f', choices=['csv', 'excel', 'both'], default='csv', help='å‡ºåŠ›å½¢å¼')
    parser.add_argument('--output', '-o', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°ãƒ­ã‚°')
    
    args = parser.parse_args()
    setup_logging(args.verbose)
    
    collections = args.collections.split(',')
    all_products = []
    
    for collection in collections:
        collection = collection.strip()
        logging.info(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection}' ã‚’å‡¦ç†ä¸­...")
        products = scrape_collection(collection, args.max_products)
        all_products.extend(products)
    
    if not all_products:
        logging.error("å–å¾—ã§ããŸå•†å“ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    logging.info(f"åˆè¨ˆ {len(all_products)} ä»¶ã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
    
    # çµ±è¨ˆã‚’è¡¨ç¤º
    df = pd.DataFrame(all_products)
    print(f"\nğŸ“Š å–å¾—çµæœçµ±è¨ˆ:")
    print(f"ğŸ¯ ç·å•†å“æ•°: {len(df)} ä»¶")
    print(f"ğŸ“¦ åœ¨åº«çŠ¶æ³åˆ†å¸ƒ:")
    for status, count in df['åœ¨åº«çŠ¶æ³'].value_counts().items():
        print(f"  {status}: {count} ä»¶")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_filename = args.output or f"chiikawa_products_{timestamp}"
    
    if args.format in ['csv', 'both']:
        csv_filename = f"{base_filename}.csv"
        save_to_csv(all_products, csv_filename)
    
    if args.format in ['excel', 'both']:
        excel_filename = f"{base_filename}.xlsx"
        save_to_excel(all_products, excel_filename)

if __name__ == "__main__":
    main()
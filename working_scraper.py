#!/usr/bin/env python3
"""
Working Chiikawa scraper based on exact debug script approach
"""

import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
from datetime import datetime
import argparse
import logging
from utils import clean_text, parse_price

def setup_logging(verbose=False):
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(level=level, format='%(asctime)s - %(levelname)s - %(message)s')

def scrape_chiikawa_products(collection_name='newitems', max_products=50):
    """Chiikawaå•†å“ã‚’å–å¾— - debug_scraperã§å‹•ä½œã—ãŸæ–¹æ³•ã‚’ä½¿ç”¨"""
    url = f"https://chiikawamarket.jp/collections/{collection_name}?page=1"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    response = requests.get(url, headers=headers)
    logging.info(f"Status code: {response.status_code}, Content length: {len(response.content)}")
    
    # Use the exact same approach as debug_scraper.py
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # debug_scraperã§æˆåŠŸã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨
    products_links = soup.find_all('a', href=re.compile(rf'/collections/{collection_name}/products/'))
    logging.info(f"Found {len(products_links)} product links")
    
    if not products_links:
        # ä»£æ›¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        all_links = soup.find_all('a', href=True)
        product_pattern_links = [link for link in all_links if '/products/' in link.get('href', '')]
        logging.info(f"Alternative search found {len(product_pattern_links)} product links")
        products_links = product_pattern_links[:max_products] if product_pattern_links else []
    
    products = []
    
    for i, link in enumerate(products_links[:max_products]):
        try:
            href = link.get('href', '')
            if href.startswith('/'):
                product_url = f"https://chiikawamarket.jp{href}"
            else:
                product_url = href
            
            # å•†å“ID
            product_id = href.split('/')[-1] if '/' in href else f"product_{i+1}"
            
            # å•†å“å
            title_text = clean_text(link.get_text()) if link.get_text().strip() else "å•†å“åä¸æ˜"
            
            # ä¾¡æ ¼ã‚’è¦ªè¦ç´ ã‹ã‚‰æ¢ã™
            price = None
            current = link.parent
            for _ in range(10):  # æœ€å¤§10éšå±¤ã¾ã§é¡ã‚‹
                if current:
                    text_content = current.get_text()
                    if 'Â¥' in text_content:
                        price = parse_price(text_content)
                        if price:
                            break
                    current = current.parent
                else:
                    break
            
            # åœ¨åº«çŠ¶æ³ã®åˆ¤å®š
            stock_status = "åœ¨åº«ã‚ã‚Š"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            parent_text = link.parent.get_text().lower() if link.parent else ""
            
            if any(keyword in parent_text for keyword in ['å£²ã‚Šåˆ‡ã‚Œ', 'sold out', 'å®Œå£²', 'åœ¨åº«ãªã—']):
                stock_status = "å£²ã‚Šåˆ‡ã‚Œ"
            elif any(keyword in parent_text for keyword in ['äºˆç´„', 'pre-order']):
                stock_status = "äºˆç´„å•†å“"
            
            # newitems ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å•†å“ã¯æ–°ç€å•†å“ã¨ã¿ãªã™
            if collection_name == 'newitems':
                stock_status = "æ–°ç€å•†å“"
            
            product = {
                'å•†å“ID': product_id,
                'å•†å“å': title_text,
                'å•†å“URL': product_url,
                'ä¾¡æ ¼': price,
                'ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³': collection_name,
                'åœ¨åº«çŠ¶æ³': stock_status,
                'å–å¾—æ—¥æ™‚': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            products.append(product)
            logging.debug(f"Product {i+1}: {title_text} - Â¥{price} ({stock_status})")
            
        except Exception as e:
            logging.warning(f"Error processing product {i+1}: {str(e)}")
            continue
    
    return products

def export_data(products, format_type, filename=None):
    """ãƒ‡ãƒ¼ã‚¿ã‚’æŒ‡å®šã•ã‚ŒãŸå½¢å¼ã§å‡ºåŠ›"""
    if not products:
        logging.warning("å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_filename = filename or f"chiikawa_products_{timestamp}"
    
    df = pd.DataFrame(products)
    
    if format_type in ['csv', 'both']:
        csv_file = f"{base_filename}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        logging.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›: {csv_file}")
    
    if format_type in ['excel', 'both']:
        excel_file = f"{base_filename}.xlsx"
        
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿
            df.to_excel(writer, sheet_name='å•†å“ãƒ‡ãƒ¼ã‚¿', index=False)
            
            # çµ±è¨ˆæƒ…å ±
            stats = []
            stats.append(['é …ç›®', 'å€¤'])
            stats.append(['ç·å•†å“æ•°', len(df)])
            stats.append(['å–å¾—æ—¥æ™‚', datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
            stats.append(['', ''])
            stats.append(['åœ¨åº«çŠ¶æ³', 'å•†å“æ•°'])
            
            for status, count in df['åœ¨åº«çŠ¶æ³'].value_counts().items():
                stats.append([status, count])
            
            if not df['ä¾¡æ ¼'].isna().all():
                stats.extend([
                    ['', ''],
                    ['ä¾¡æ ¼çµ±è¨ˆ', ''],
                    ['å¹³å‡ä¾¡æ ¼', f"Â¥{df['ä¾¡æ ¼'].mean():.0f}"],
                    ['æœ€é«˜ä¾¡æ ¼', f"Â¥{df['ä¾¡æ ¼'].max():.0f}"],
                    ['æœ€ä½ä¾¡æ ¼', f"Â¥{df['ä¾¡æ ¼'].min():.0f}"]
                ])
            
            stats_df = pd.DataFrame(stats)
            stats_df.to_excel(writer, sheet_name='çµ±è¨ˆæƒ…å ±', index=False, header=False)
        
        logging.info(f"Excelãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›: {excel_file}")

def main():
    parser = argparse.ArgumentParser(description='ã¡ã„ã‹ã‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒãƒ¼ã‚±ãƒƒãƒˆ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«')
    parser.add_argument('--collections', '-c', default='newitems', help='å–å¾—ã™ã‚‹ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³')
    parser.add_argument('--max-products', '-m', type=int, default=50, help='æœ€å¤§å•†å“æ•°')
    parser.add_argument('--format', '-f', choices=['csv', 'excel', 'both'], default='csv', help='å‡ºåŠ›å½¢å¼')
    parser.add_argument('--output', '-o', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°ãƒ­ã‚°')
    
    args = parser.parse_args()
    setup_logging(args.verbose)
    
    collections = [c.strip() for c in args.collections.split(',')]
    all_products = []
    
    for collection in collections:
        logging.info(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection}' ã‚’å‡¦ç†ä¸­...")
        products = scrape_chiikawa_products(collection, args.max_products)
        all_products.extend(products)
        logging.info(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{collection}': {len(products)} å•†å“ã‚’å–å¾—")
    
    if not all_products:
        logging.error("å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # çµæœçµ±è¨ˆ
    df = pd.DataFrame(all_products)
    print(f"\nğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœ:")
    print(f"ğŸ¯ ç·å•†å“æ•°: {len(df)} ä»¶")
    print(f"ğŸ“¦ åœ¨åº«çŠ¶æ³åˆ†å¸ƒ:")
    for status, count in df['åœ¨åº«çŠ¶æ³'].value_counts().items():
        print(f"  {status}: {count} ä»¶")
    
    if not df['ä¾¡æ ¼'].isna().all():
        print(f"ğŸ’° ä¾¡æ ¼çµ±è¨ˆ:")
        print(f"  å¹³å‡ä¾¡æ ¼: Â¥{df['ä¾¡æ ¼'].mean():.0f}")
        print(f"  ä¾¡æ ¼ç¯„å›²: Â¥{df['ä¾¡æ ¼'].min():.0f} - Â¥{df['ä¾¡æ ¼'].max():.0f}")
    
    # ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›
    export_data(all_products, args.format, args.output)

if __name__ == "__main__":
    main()